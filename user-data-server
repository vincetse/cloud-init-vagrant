#cloud-config

apt_update: true
apt_upgrade: true
apt_reboot_if_required: true

apt_sources:

# Docker
- source: "deb [arch=amd64] https://download.docker.com/linux/ubuntu zesty stable"
  filename: "docker.list"
  keyid: "0EBFCD88"

- source: "ppa:gluster/glusterfs-3.12"
  filename: "glusterfs.list"

packages:
- glusterfs-server
- lvm2
- sudo
- ssh-import-id
- sysstat
- thin-provisioning-tools

ssh_authorized_keys:
- ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA6NF8iallvQVp22WDkTkyrtvp9eWW6A8YVr+kz4TjGYe7gHzIw+niNltGEFHzD8+v1I2YJ6oXevct1YeS0o9HZyN1Q9qgCgzUFtdOKLv6IedplqoPkcmF0aYet2PkEDo3MlTBckFXPITAMzF8dJSIFo9D8HfdOV0IAdx4O7PtixWKn5y2hMNG0zQPyUecp4pzC6kivAIhyfHilFR61RGL+GPXQ2MWZWFYbAGjyiYJnAmCP3NOTd0jMZEnDkbUvxhMmBYSdETk1rRgm+R4LOzFUGaHqHDLKLX+FIPKcF96hrucXzcWyLbIbEgE98OHlnVYCzRdK8jlqm8tehUc9c9WhQ== vagrant insecure public key

write_files:
- path: "/etc/hosts"
  permissions: "0644"
  owner: "root:root"
  content: |
    127.0.0.1 localhost
    10.100.0.50 gclient01
    10.100.0.51 g01
    10.100.0.52 g02
    10.100.0.53 g03

- path: "/tmp/provision.sh"
  permissions: "0744"
  owner: "root:root"
  content: |
    #!/bin/bash -eu
    ################################################################################
    # LVM setup
    # http://docs.gluster.org/en/latest/Administrator%20Guide/Setting%20Up%20Volumes/#setting-up-glusterfs-server-volumes
    # https://www.tecmint.com/setup-thin-provisioning-volumes-in-lvm/
    ################################################################################
    # We added two 512GB drives in the Vagrantfile, and they are added as these
    # devices by Linux
    function setup_lvm()
    {
      local gfs_volume_group_name=$1
      local gfs_thin_pool_name=$2
      local gfs_thin_volume_name=$3
      local gfs_bricks_base=$4
      local disks=$5
      # Remove any existing partition tables
      for disk in ${disks}; do
        dd if=/dev/zero of=${disk} bs=512 count=1
      done
      # Create an LVM Thin Pool as GFS server volume
      (
        set -ex
        pvcreate --dataalignment 1280K ${disks}
        vgcreate --physicalextentsize 128K ${gfs_volume_group_name} ${disks}
        vgs
        lvcreate --size 999G --thinpool ${gfs_thin_pool_name} ${gfs_volume_group_name}
        lvdisplay ${gfs_volume_group_name}/${gfs_thin_pool_name}
        lvchange --zero n ${gfs_volume_group_name}/${gfs_thin_pool_name}
        lvcreate --virtualsize 10G --thin --name ${gfs_thin_volume_name} ${gfs_volume_group_name}/${gfs_thin_pool_name}
        lvs
        mkfs.xfs -f -i size=512 -n size=8192 -d su=128k,sw=10 /dev/${gfs_volume_group_name}/${gfs_thin_volume_name}
      )
    }

    # The drives available for GlusterFS
    DISKS="/dev/sdb /dev/sdc"
    # The LVM volume group that we are creating for the GFS volume
    GFS_VOLUME_GROUP_NAME=vg_gfs
    # LVM Thin Pool for managing LVM volumes.
    GFS_THIN_POOL_NAME=tp_gfs
    # LVM Thinly-Provisioned volume that lets us over subscribe to the space we have.
    GFS_THIN_VOLUME_NAME=gfs_volume
    # The base directory to the mountpoints for the LVM volumes
    # https://gluster.readthedocs.io/en/latest/Administrator%20Guide/Brick%20Naming%20Conventions/
    GFS_BRICKS_BASE=/data/glusterfs
    setup_lvm ${GFS_VOLUME_GROUP_NAME} ${GFS_THIN_POOL_NAME} ${GFS_THIN_VOLUME_NAME} ${GFS_BRICKS_BASE} "${DISKS}"
    # The mountpoint for the LVM volume just created to be used as the GlusterFS server volume
    BRICK1=${GFS_BRICKS_BASE}/${GFS_THIN_VOLUME_NAME}/brick1

    # data directory for bricks
    (
      set -ex
      mkdir -p ${BRICK1}
      echo "/dev/${GFS_VOLUME_GROUP_NAME}/${GFS_THIN_VOLUME_NAME} ${BRICK1} xfs rw,inode64,noatime,nouuid 0 0" >> /etc/fstab
      mount ${BRICK1}
      mount
    )

    ################################################################################
    # GlusterFS setup
    ################################################################################
    function probe_peer()
    {
      # keep probing for peer until we find it
      local peer=$1
      while :; do
        set +e
        (
          set -ex
          gluster peer probe ${peer}
        )
        local retv=$?
        set -e
        [[ "${retv}" == 0 ]] && break
        sleep 1
      done
    }

    # Probe for GlusterFS peers and set up volume
    if [[ "$(hostname)" == "g01" ]]; then
      probe_peer g02
      probe_peer g03
      # It seems like we need to stop for a short time for the peers to arrive
      # at a quorum of some sort.
      sleep 5
      (
        set -ex
        gluster volume create ${GFS_THIN_VOLUME_NAME} replica 3 g0{1..3}:${BRICK1}/brick
        gluster volume start ${GFS_THIN_VOLUME_NAME}
        gluster volume info ${GFS_THIN_VOLUME_NAME}
      )
    fi

runcmd:
- /tmp/provision.sh
